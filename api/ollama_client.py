# src/api/ollama_client.py

import ollama
import json
from src.core.settings import settings

class OllamaClient:
    """
    ä¸€ä¸ªç”¨äºä¸æœ¬åœ°OllamaæœåŠ¡äº¤äº’çš„APIå®¢æˆ·ç«¯ã€‚
    """
    def __init__(self, api_base=None, api_key=None): # api_key åœ¨è¿™é‡Œæ˜¯å¯é€‰çš„ï¼Œä¸ºäº†æ¥å£ç»Ÿä¸€
        # ä»è®¾ç½®ä¸­è·å–Ollamaçš„åŸºç¡€URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.api_base = settings.get('Ollama/api_base', 'http://localhost:11434')
        
        # ä½¿ç”¨å®˜æ–¹çš„ ollama Python åº“æ¥åˆ›å»ºä¸€ä¸ªå®¢æˆ·ç«¯å®ä¾‹
        try:
            self.client = ollama.Client(host=self.api_base)
            # å°è¯•åˆ—å‡ºæ¨¡å‹ä»¥éªŒè¯è¿æ¥
            self.client.list() 
            print(f"Ollama å®¢æˆ·ç«¯æˆåŠŸè¿æ¥åˆ°: {self.api_base}")
        except Exception as e:
            print(f"!!! è¿æ¥ Ollama æœåŠ¡å¤±è´¥: {e}")
            print("!!! è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸” API Base URL é…ç½®æ­£ç¡®ã€‚")
            self.client = None

    def list_models(self) -> dict | None:
        """ä»OllamaæœåŠ¡è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚"""
        if not self.client: return None
        try:
            models_data = self.client.list().get('models', [])
            processed_models = {}
            for model in models_data:
                model_id = model.get('name')
                if model_id:
                    # Ollamaæ¨¡å‹é€šå¸¸éƒ½æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå–å†³äºå…·ä½“æ¨¡å‹ï¼‰å’Œè§†è§‰ï¼ˆå¦‚æœæ¨¡å‹æ˜¯å¤šæ¨¡æ€çš„ï¼‰
                    # æˆ‘ä»¬å¯ä»¥æ ¹æ®æ¨¡å‹IDä¸­çš„å…³é”®å­—æ¥åšä¸€ä¸ªç®€å•çš„åˆ¤æ–­
                    supports_tools = "instruct" in model_id.lower() or "function" in model_id.lower()
                    is_vision = "llava" in model_id.lower() or "bakllava" in model_id.lower()
                    
                    display_name = model_id.replace(":", " ").title()
                    if supports_tools: display_name += " ğŸ› ï¸"
                    if is_vision: display_name += " ğŸ–¼ï¸"
                    
                    processed_models[display_name] = {
                        "id": model_id,
                        "supports_tools": supports_tools,
                        "is_vision": is_vision
                    }
            return processed_models
        except Exception as e:
            print(f"ä»Ollamaè·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_chat_response_stream(self, messages: list, model_name: str, **kwargs):
        """
        ä»Ollamaè·å–èŠå¤©å›å¤çš„æµå¼ç”Ÿæˆå™¨ã€‚
        æ³¨æ„ï¼šOllamaçš„ 'tools' å‚æ•°æ ¼å¼ä¸OpenAI APIä¸åŒï¼Œæˆ‘ä»¬æš‚æ—¶åªæ”¯æŒçº¯æ–‡æœ¬ã€‚
        """
        if not self.client:
            # è¿”å›ä¸€ä¸ªç©ºçš„ç”Ÿæˆå™¨
            yield {}
            return

        try:
            # Ollama å®¢æˆ·ç«¯çš„ stream æ–¹æ³•ç›´æ¥è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨
            stream = self.client.chat(
                model=model_name,
                messages=messages,
                stream=True
            )
            
            # æˆ‘ä»¬éœ€è¦å°†Ollamaçš„å“åº”æ ¼å¼ï¼ŒåŒ…è£…æˆæˆ‘ä»¬ç¨‹åºæœŸæœ›çš„OpenAIå…¼å®¹æ ¼å¼
            for chunk in stream:
                delta_content = chunk.get('message', {}).get('content', '')
                if delta_content:
                    yield {
                        "choices": [{
                            "delta": {"content": delta_content}
                        }]
                    }
        except Exception as e:
            print(f"è°ƒç”¨Ollamaæ¨¡å‹æ—¶å‡ºé”™: {e}")
            yield {
                "choices": [{
                    "delta": {"content": f"\n\n[Ollama Error]: {e}"}
                }]
            }
